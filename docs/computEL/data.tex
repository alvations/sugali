\section{The Data}\label{sec:data}

\subsection{Representation and Universality} \label{sec:stats}

According to Ethnologue, there are 7105 living languages, and 147 living language families. Across all our data sources, we manage to cover 1347 languages in 106 families, which represents 19.0\% of the world's languages. To get a better idea of the kinds of languages represented, we give a breakdown according to their EGIDS scores (Expanded Graded Intergenerational Disruption Scale) in figure \textcolor{blue}{[insert reference here when it's ready]}. The values in each cell have been colored according to proportion of languages represented, with green indicated good coverage and red poor. It's interesting to note that vigorous languages (6a) are poorly represented across all data sources, and worse than more endangered categories. In terms of language documentation, vigorous languages are less urgent goals than those in categories 6b and up, but this highlights an unexpected gap in linguistic resources.



%\begin{table}
%\centering
%    \begin{tabular}{l|cc}
%    ~         				& \#Languages & \#Families \\ \hline
%    ODIN      				& 1218       & 101       \\
%    Omniglot  				& 134        & 21        \\
%    UDHR      				& 355        & 47        \\
%    Wikipedia 				& 209        & 22       \\ \hline
%    \textbf{Combined}	& 1347			 & 106 
%    \end{tabular}
%\caption{Corpus Coverage}
%\end{table}


\begin{table}[h!]
\centering
    \begin{tabular}{l|rr|rr}
    ~         				& \#Languages & \#Families 	&\#tokens		& Size	\\ \hline
    ODIN      				& 1,217      & 101       		& 58,556		& 39 MB		\\
    Omniglot  				& 134        & 21        		&	6,749			& 677 KB	\\
    UDHR      				& 355        & 47        		&	33,117		& 5.2 MB	\\
    Wikipedia 				& 209        & 22       		&						& 37 GB		\\ \hline
    \textbf{Combined}	& 1,347			 & 106 
    \end{tabular}
\caption{Corpus Coverage}
\label{table:corpus}
\end{table}

\textcolor{blue}{[insert heatmap here]}



\subsection{Data Sources} \label{sec:sources}

Although data size matters in general NLP \cite{banko2001scaling} \textcolor{blue}{[is this the best reference?]}, \emph{universality} is the utmost priority for a universal corpus. We chose to focus on the following data sources, because they include a large number of languages, include several parallel texts, and demonstrate a variety of data types which a linguist might encounter (structured, semi-structured, unstructured):

\begin{enumerate}
\item The Online Database of Interlinear Text (ODIN)
\item The Omniglot website
\item The Universal Declaration of Human Rights (UDHR)
\item Wikipedia
\end{enumerate}


Our resulting corpus runs the full gamut of text types outlined by Abney and Bird, ranging from single-language text (Wikipedia and LCC) to parallel text (UDHR and Omniglot) to IGTs (ODIN).


\subsection{Universal Corpus and Data Structure} \label{sec:structure}

\textcolor{blue}{[Perhaps this comes elsewhere?]}

\newcite{abney2011data} describe the data structure they envisage for the universal corpus in more detail, distinguishing between \textbf{aligned texts} and \textbf{analysed texts}: the former consists of multiple parallel texts, aligned at the document, sentence, or word level [add more detail]; the latter contains more detailed annotations including parts of speech, morphological information, and syntactic relations. In our work, we have strived to maintain a data structure consistent with their recommendations for aligned texts.

However, we disagree that analysed texts should be accommodated in this way. If such a universal corpus is to succeed at all, it must enjoy support from a substantial part, if not all, of the linguistics community, and doing this requires theory-neutrality. Although their data structure is fairly lightwight, it is not theory neutral, explicitly encoding support for dependency grammars but not constituency grammars, and positing a specific list of relevant features which should be used to annotate words. Other linguists might wish to use a different set of properties, which could threaten to fragment a universal corpus before it truly gets off the ground. More fundamentally, they assume that the corpus should be segmented into words, but \newcite{haspelmath2011segment} argues that there is no cross-linguistically valid definition of "word", which would render such an endeavour impossible from the start.

It is not within the scope of this paper to resolve these theoretical concerns. Instead, we suggest that the data structure for a universal corpus should be even more lightweight than Abney and Bird suggest. We agree with their characterisation of aligned texts, but propose an alternative for analysed texts. To motivate the role of parallel texts in a universal corpus, they propose using translations into a high-resource reference language as a convenient surrogate of meaning. By the same reasoning, we can use the glosses in an IGT to provide a more detailed surrogate of meaning. We propose that, just as we can align texts at the document, sentence, and word levels, we can also align texts at the morpheme level, as recommended by the Leipzig Glossing Rules,\footnote{http://www.eva.mpg.de/lingua/resources/glossing-rules.php} and as practised by documentary linguists.

Then, only difference between analysed and aligned texts is that the text is aligned with descriptions in a metalanguage,\footnote{While we would urge researchers to keep such annotations simple, as usually done in IGTs, there is nothing in principle to stop someone from using a complex metalanguage capable of encoding the full set of properties proposed by Abney and Bird, or even more complicated data structures.} rather than a natural language. The benefits, however, are that we can use the same data structure to represent both aligned and analysed texts, and the representation is simple enough that linguistic controversies are difficult to raise. We admit that highly detailed annotations, such as trees and dependency graphs, are more awkward to encode, but such resources are unlikely to be available in more than a small handful of languages, and will remain outside the scope of a universal corpus, at least for the foreseeable future. To make it indicate what reference language is being used, we propose prefixing \texttt{gloss-} to the language code of the reference language - for example \texttt{gloss-eng} and \texttt{gloss-eng} if a text has been glossed into English or Spanish, respectively. Using a prefix rather than a suffix makes it clear that this is not a subvariety of the given language.

Finally, it is important to make sure that the data we have compiled will be available to future researchers, regardless of how the surrounding infrastructure changes. \newcite{bird2003port} describes a set of best practices for maintaining portability of digital information, outlining "seven dimensions" along which this can vary. [more detail?] Following this advice, we have ensured that all our data is available as plain text files, with utf-8 encoding, labelled with the relevant ISO 639-3 code. We have written an API to allow access to this data according to the guidelines of Abney and Bird, who remain agnostic as to the specific form of data storage. If, for reasons of space or speed, an alternative format would be preferred, the data would be straightfoward to convert since it can be accessed according these guidelines.


