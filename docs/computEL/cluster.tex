\section{Detecting Similar Languages} \label{sec:cluster}

When working on low-resource or endangered languages, computational and documentary linguists face the issue of lack of resources and knowledge about the language. Often, having knowledge about related or similar languages provides useful lexical, syntactic or morphological minimal pairs across languages and also helps in bootstrapping language models in NLP \cite{yarowsky:ngai:2001,xia2007multilingual}.

Language classification can be carried out either: geneologically, mapping languages onto families trees depending on their historical ancenstry \cite{swadesh1952,starostin2010}; or typologically, grouping languages according to typological features \cite{georgi2010wals,daume2009}.

To exemplify the use of the universal corpus for research on low-resource languages, we experimented with automatic detection of similar languages using hierarchical clustering with character ngrams and word unigrams features. Each language is represented by a vector of character bigrams and trigrams and word unigram features from the universal corpus. 
\newline \newline
\noindent \textbf{SHOW FANCY FIGURE OF CLUSTER}
\newline \newline
\textbf{Evaluate the cluster (quantitatively/qualitatively)}


\begin{table*}[h!]
\begin{centering}

    \begin{tabular}{l|ccc|ccc}
    ~        & complete & ~       & ~       & ward    & ~       & ~       \\ \cline{2-7}
    ~        & precision & recall       & f-score       & precision    & recall       & f-score      \\ \hline
    distance & 0.0614   & 0.8565 & \textbf{0.1099} & 0.06140  & 0.8565 & \textbf{0.1099} \\
    maxclust & 0.1925  & \textbf{0.0927} & 0.0692 & \textbf{0.1963} & 0.0905 & 0.0686  \\
    \end{tabular}
\caption{Comparing of clustering algorithms where number of clusters is set to genetic grouping on Ethnologue.}
\end{centering}
\label{tab:cluster}
\end{table*}


\textcolor{blue}{[Use the above figures as a random baseline, and insert the new figures]}


\noindent
see full table in http://goo.gl/bs2vPo





There are many possible metrics to evaluate the quality of a clustering compared to a gold standard. \newcite{amigo2009metrics} propose a set of criteria which a clustering evaluation metric should satisfy, and demonstrate that most popular metrics fail to satisfy at least one of these criteria.  However, they prove that they are satisfied by the BCubed metric, which we adopt for this reason.  To calculate this, we find the induced cluster and gold standard class for each language, and calculate the F-score of the cluster compared to the class.  These F-scores are then averaged across all languages.

As well as performing clustering, we can view this as an information retrieval task: given a language, what are similar languages?  To do this, we found the languages with the closest vectors of n-grams and words.  Since BCubed is calculated averaging across languages and not clusters, we can use exactly the same calculation, using the set of nearby languages in place of a cluster.

In table \ref{tab:cluster}, we give results of our clustering experiments.  The f-scores are much higher than the random baseline, and comparable to the values reported by \newcite{georgi2010wals}, even though we have only used surface features, while they used typological features taken from WALS.

It is also worth noting that precision is higher than recall.  This is perhaps to be expected, given that related languages using wildly differing orthographies will appear to be very different.  Nonetheless, our system is reasonably capable of identifying those languages which are both related and also written similarly.
