\section{Data Clean-Up, Consistency, and Standardisation} \label{sec:case_studies}
Consistency in data structures and formatting is essential to facilitate use of data in computational linguistics research \cite{palmer2010lilt}. In the following subsections, we describe the processing required to convert the data into a standardised form.  We then discuss standardisation of language codes and file formats.

\subsection{Case Studies}


\paragraph{UDHR.} Although there is a pre-compiled version of the UDHR data from the
Natural Language ToolKit (NLTK) corpora
distribution,\footnote{\url{http://nltk.googlecode.com/svn/trunk/nltk_data/index.xml}}
the distribution is laden with encoding problems. 
%during their
%conversion from pdf to plaintext format. 
Instead, we use the
plaintext files available from the Unicode
website,\footnote{\url{http://unicode.org/udhr/d}} which are free of
encoding issues. The first four lines of each file record metadata,
and the rest is the translation of the UDHR. This dataset is
extremely clean, and simply required segmentation into sentences.


\paragraph{Wikipedia.}
One major issue with using the Wikipedia dump is the problem of separating text from abundant source-specific markup. To convert compressed
Wikipedia dumps to textfiles, we used the WikiExtractor\footnote{\url{http://medialab.di.unipi.it/wiki/Wikipedia_Extractor}}
tool. After conversion into textfiles, we used several regular expressions to
delete residual Wikipedia markup and so-called ``magic
words"\footnote{\url{http://en.wikipedia.org/wiki/Help:Magic_words}}.


\paragraph{Omniglot.}
The main issue with extracting the Omniglot data is that the pages are designed to be human-readable, not machine-readable.  Cleaning this data required parsing the HTML source, and extracting the relevant content, which required different code for the two types of page we considered ('\emph{Useful foreign phrases}' and '\emph{Tower of Babel}').  Even after automatic extraction, some noise in the data remained, such as explanatory notes given in parentheses, which are written in English and not the target language.  Even though the total amount of data here is small compared to our other sources, the amount of effort required to process it was not, because of these idiosyncracies.  We expect that researchers seeking to convert data from human-readable to machine-readable formats will encounter similar problems, but unfortunately there is unlikely to be a one-size-fits-all solution to this problem.


\paragraph{ODIN.}
The ODIN data is easily accessible in XML format from the online database\footnote{\url{http://odin.linguistlist.org/download}}. Data for each language is saved in a separate XML file and the IGTs are encoded in tags of the form \texttt{<igt><example>...</example></igt>}.  For example, the IGT in figure~\ref{fig:odin_fijian} is represented by the XML snippet in figure~\ref{fig:odin_fijian_xml}.

% \begin{minipage}{\columnwidth}
% \quad 21 a.\quad o lesu mai \\
% \indent \qquad\qquad 2sg return here \\
% \indent \qquad\qquad `\emph{You return here.}' \\
% %\caption{Fijian IGT from ODIN.} 
% \end{minipage}



The primary problem in extracting the data is a lack of consistency in
the IGTs. In the above examples, the sentence is introduced by a
letter or number, which needs to be removed; however, the form of such
indexing elements varies. In addition, the source line in figure~\ref{fig:odin_yimas_xml}
includes two types of metadata: the language name, and a citation,
both of which introduce noise.  Finally, extraneous punctuation such
as the quotation marks in the translation line need to be removed. We used regular expressions for cleaning lines within the IGTs.


\begin{figure}[t]
\quad 21 a.\quad o lesu mai \\
\indent \qquad\qquad 2sg return here \\
\indent \qquad\qquad `\emph{You return here.}' \\
\caption{Fijian IGT from ODIN} \label{fig:odin_fijian}
\end{figure}


\begin{figure}[t]
\small
\begin{lstlisting}[language=XML]
<igt>
  <example>
    <line>21 a. o lesu mai</line>
    <line>2sg return here</line>
    <line>`You return here.'</line>
  </example>
</igt>
\end{lstlisting} 
\caption{Fijian IGT in ODIN's XML format} \label{fig:odin_fijian_xml}
\end{figure}

% \begin{minipage}{\columnwidth}
% \begin{lstlisting}[language=XML]
% <igt>
%   <example>
%     <line>21 a. o lesu mai</line>
%     <line>2sg return here</line>
%     <line>`You return here.'</line>
%   </example>
% </igt>
% \end{lstlisting} 
% \centerline{Figure 2: Fijian IGT in ODIN's XML format.}
% \end{minipage}



\begin{figure}[t]
\small
\begin{lstlisting}[language=XML]
<igt>
  <example>
    <line>(69) na-Na-tmi-kwalca-t 
    Yimas (Foley 1991)</line>
    <line>3sgA-1sgO-say-rise-PERF
    </line>
    <line>`She woke me up' 
    (by verbal action)</line>
  </example>
</igit>
\end{lstlisting} 
\smallskip
\caption{Yimas IGT in ODIN's XML format}\label{fig:odin_yimas_xml}
\end{figure}

% \begin{minipage}{\columnwidth}
% \begin{lstlisting}[language=XML]
% <igt>
%   <example>
%     <line>(69) na-Na-tmi-kwalca-t 
%     Yimas (Foley 1991)</line>
%     <line>3sgA-1sgO-say-rise-PERF
%     </line>
%     <line>`She woke me up' 
%     (by verbal action)</line>
%   </example>
% </igit>
% \end{lstlisting} 
% \smallskip
% \centerline{Figure 3: Yimas IGT in ODIN's XML format.}
% \end{minipage}


% To clean the source lines, we used the following regular expressions:

% \begin{figure}
% \begin{itemize}
% \item \emph{Cleaner}: Removed (i) all heading and trailing text embedded in square or rounded brackets and (ii) heading double character token ending with bracket or fullstop.
% \begin{itemize}
% \item[(i)]
% \begin{Verbatim}
% ^(?\s?\w{1,5}\s*[):.]\s*
% \end{Verbatim}
% \item[(ii)] 
% \begin{Verbatim}
% [\[\(].{1,}[\]\)]
% \end{Verbatim}
% \end{itemize}
% \item \emph{Cleanest}: Only source lines without punctuation.
% \end{itemize}
% \caption{I don't think we want this as a figure.}
% \end{figure}


% \begin{minipage}{\columnwidth}
% \begin{itemize}
% \item \emph{Cleaner}: Removed (i) all heading and trailing text embedded in square or rounded brackets and (ii) heading double character token ending with bracket or fullstop.
% \begin{itemize}
% \item[(i)]
% \begin{Verbatim}
% ^(?\s?\w{1,5}\s*[):.]\s*
% \end{Verbatim}
% \item[(ii)] 
% \begin{Verbatim}
% [\[\(].{1,}[\]\)]
% \end{Verbatim}
% \end{itemize}
% \item \emph{Cleanest}: Only source lines without punctuation.
% \end{itemize}
% \end{minipage}


% The original version of the ODIN data contains XML files for 1275 languages, while the cleaner version of ODIN contains IGTs for 1042 languages and the cleanest version contains IGTs for 402 languages.  \textcolor{blue}{Maybe we should just remove this next sentence:} The drop from 1275 to 1042 languages was largely because {\color{red} XXXX} XML files from the original ODIN data had used language codes that were not in ISO 639-3 and for {\color{red} XXXX} other files, the \texttt{<igt>...</igt>} tags were missing.

% In future work, we intend to explore more sophisticated methods of cleaning.  For example, we could leverage the fact that the number of hyphens and equal signs in each word should match between the source and gloss lines.  
%However, no method is likely to be foolproof.  
%As people incorporate large sets of IGTs from documentary linguistic projects into a universal corpus, we can expect problems with inconsistencies to grow.

\subsection{Language Codes}
We use ISO~639-3 as our standard set of language codes, since it aims for universal coverage, and has widespread acceptance in the community. % As a result, we needed to make sure that all our data was labelled with these codes.  
Data from ODIN and the UDHR already used this standard, and hence did not pose problems.

Wikipedia uses its own set of language codes, most of which are in ISO~639-1 or ISO~639-3.  The older ISO~639-1 codes are easy to recognise, being two letters long instead of three, and can be straightforwardly converted.  However, a small number of Wikipedia codes are not ISO codes at all - we converted these to ISO~639-3, following documentation from the Wikimedia Foundation.\footnote{\url{http://meta.wikimedia.org/wiki/Special_language_codes}}

Omniglot does not give codes at all, but only the language name. To resolve this issue, we automatically converted language names to codes using information from the SIL website.\footnote{\url{http://www-01.sil.org/iso639-3/iso-639-3.tab}}



\subsection{File Formats}

It is important to make sure that the data we have compiled will be available to future researchers, regardless of how the surrounding infrastructure changes. \newcite{bird2003port} describe a set of best practices for maintaining portability of digital information, outlining seven dimensions along which this can vary. Following this advice, we have ensured that all our data is available as plain text files, with utf-8 encoding, labelled with the relevant ISO 639-3 code. Metadata is stored separately. We have written an API to allow access to the data according to the guidelines of \newcite{abney2010universal}, who remain agnostic as to the specific form of data storage. If, for reasons of space or speed, an alternative format would be preferred, the data would be straightfoward to convert since it can be accessed according to these guidelines.
