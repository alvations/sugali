\documentclass[11pt]{article}
\usepackage{acl2014}
\usepackage{times}
\usepackage{url}
\usepackage{latexsym}

\bibliographystyle{acl}

\usepackage[utf8]{inputenc}

\usepackage{enumitem}
\usepackage{listings}
\usepackage{fancyvrb}
\usepackage{color}
\definecolor{gray}{rgb}{0.4,0.4,0.4}
\definecolor{darkblue}{rgb}{0.0,0.0,0.6}
\definecolor{cyan}{rgb}{0.0,0.6,0.6}
\definecolor{orange}{rgb}{1,0.5,0}

\lstset{
  basicstyle=\ttfamily,
  columns=fullflexible,
  showstringspaces=false,
  commentstyle=\color{gray}\upshape
}

\lstdefinelanguage{XML}
{
  morestring=[b]",
  morestring=[s]{>}{<},
  morecomment=[s]{<?}{?>},
  stringstyle=\color{black},
  identifierstyle=\color{darkblue},
  keywordstyle=\color{cyan},
  morekeywords={xmlns,version,type}% list your attributes here
}


\setlength\titlebox{5cm}

\title{Getting the Ball Rolling: Producing a Foundation Text for a Universal Corpus of the World's Languages}

\author{Liling Tan, Guy Emerson, Susanne Fertmann, Alexis Palmer and Michaela Regneri \\
  Universität des Saarlandes \\
  Campus, 66123 Saarbrücken, Germany \\
  {\tt s9sufert@stud.uni-saarland.de, emerson@coli.uni-saarland.de,} \\
  {\tt liling.tan@uni-saarland.de}}

\date{}

\begin{document}
\maketitle
\begin{abstract}
The study of endangered languages is limited by a lack of data. Existing corpora collections are limited in the range of languages covered, in standardisation, or in machine-readability. This makes the situation even worse for the computational linguist, especially one who would like to take a cross-linguistic or typological approach. We first survey existing efforts to compile cross-linguistic resources, then describe our own approach and give an example application - language clustering. To build the foundation text for a Universal Corpus, we crawled and cleaned texts from several web sources that contain data from a large number languages, and converted them into a standardised form consistent with the guidelines set out by \newcite{abney2010universal}. The resulting corpus is more easily-accessible and machine-readable than any of the underlying data sources, and represents a significant base corpus for researchers to draw on and add to in the future.
\end{abstract}

\section{Introduction}

\newcite{abney2010universal} posed the grand challenge of building a Universal Corpus, including all of the world's languages, in a consistent structure.

7105 living languages.


\section{Related Work}

[...]
Currently, multilingual corpora efforts have limited coverage in the number of languages and the number of language families represented. For instance, the OPUS corpus \cite{tiedemann2012opus} covers over 90 languages, only 1.27\% of the world's languages; The Leipzig Corpora Collection\footnote{corpora.uni-leipzig.de} \cite{biemann2007leipzig} contains corpora and dictionaries in 230 languages, 3.24\% of the world's languages. Even corpora that boast of linguistic diversity include only a small number of language families, e.g. the linguistically diverse NTU-Multilingual Corpus \cite{tan2011ntu} covers only 7 out of 136 language families. Producing a universal corpus requires the linguistic community to merge existing corpora and provide an ubiquitous access interface. To compile the foundation text for the Universal Corpus, we crawled and cleaned web data that contains multilingual texts and merged them with existing corpora collections to form the foundation text for the Universal Corpus.


\subsection{Web as Corpus}

\newcite{baroni2004bootcat} introduced the BootCaT toolkit to bootstrap specialized corpora and terms from the web. Then, \newcite{sharoff2006search} built corpora in various languages by issuing queries for random combinations of frequent words to create a balanced corpus not unlike the British National Corpus. Later, \newcite{ferraresi2008ukwac} introduced and discussed the notion of compiling Web as Corpus \emph{"properly"} with the inaugural ukWaC using web-crawled data from a list of seed words. Thereafter, the Crúbadán corpora was created for a large number of under-resourced languages \cite{scannell2007crubadan}. Following which the deWaC (German), itWaC (Italian) and frWaC (French) were compiled \cite{baroni2009wacky}. 

Subsequently, \newcite{brunello2009free} stressed the notion of building free corpora from the web using documents released under Creative Commons licenses. To build better quality WaCky corpora, \newcite{versley2012quality} introduced the notion of content-sensitive boilerplate detection in cleaning web corpora. 

The initial wave of monolingual WaCky corpora \cite{baroni2009wacky} inspired more domain specific WaCky corpora, such as the Korean Web Learner's Corpus \cite{dickinson2010korean} and the Feed Corpus \cite{minocha2013feed}. Meanwhile, Web-driven corpora compilations also focused on building large  multilingual resources, e.g. The Leipzig Corpora Collection (\texttt{LCC}) \cite{biemann2007leipzig}, and the COrpora from the Web (\texttt{COW}) project \cite{schaefer2012cow}.

Unlike previous WaCs built under the Web as Corpus kool ynitiative (WaCky), we did not perform seed query web-searching or web crawling to achieve a balanced resource. Instead, the foundation text for uniWaC was built using dedicated site crawling and/or site-dependent cleaning from four data sources, viz. (i) \emph{Online Database of Interlinear Text} (ODIN), (ii) \emph{Omniglot} website, (iii) \emph{Universal Declaration of Human Rights} (UDHR) and (iv) \emph{Wikipedia} dumps. Our emphasis for the uniWaC is based on based on \textbf{\emph{universality}}; covering as many languages as possible is the first priority.



\section{Data Structure}

\newcite{abney2011data} describe the data structure they envisage for the universal corpus in more detail, distinguishing between \textbf{aligned texts} and \textbf{analysed texts}: the former consists of multiple parallel texts, aligned at the document, sentence, or word level; the latter contains more detailed annotations including parts of speech, morphological information, and syntactic relations. In our work, we have strived to maintain a data structure consistent with their recommendations for aligned texts.

However, we disagree that analysed texts should be accommodated in this way. If such a universal corpus is to succeed at all, it must enjoy support from the entire linguistics community, and doing this requires complete theory-neutrality. Although their data structure is fairly lightwight, it is not theory neutral, explicitly encoding support for dependency grammars but not constituency grammars, and positing a specific list of relevant features which should be used to annotate words. Other linguists might wish to use a different set of properties, which could threaten to fragment a universal corpus before it truly gets off the ground. More fundamentally, they assume that the corpus should be segmented into words, but \newcite{haspelmath2011segment} argues that there is no cross-linguistically valid definition of "word", which would render such an endeavour impossible from the start.

It is not within the scope of this paper to resolve these theoretical concerns. Instead, we suggest that the data structure for a universal corpus should be even more lightweight than Abney and Bird suggest. We agree with their characterisation of aligned texts, but propose an alternative for analysed texts. To motivate the role of parallel texts in a universal corpus, they propose using translations into a high-resource language as a convenient surrogate of meaning. By the same reasoning, we can use the glosses in an IGT to provide a more detailed surrogate of meaning. We propose that, just as we can align texts at the document, sentence, and word levels, we can also align texts at the morpheme level, as recommended by the Leipzig Glossing Rules.\footnote{http://www.eva.mpg.de/lingua/resources/glossing-rules.php} The only difference here is that a text is aligned with descriptions in a metalanguage,\footnote{While we would urge researchers to keep such annotations simple, as usually done in IGTs, there is nothing in principle to stop someone from using a complex metalanguage capable of encoding the full set of properties proposed by Abney and Bird, or even more complicated data structures.} rather than a natural language. The benefits, however, are that we can use the same data structure to represent both aligned and analysed texts, and the representation is simple enough that linguistic controversies cannot be raised. We admit that highly detailed annotations, such as in treebanks and dependency banks, are more awkward to encode, but such resources are unlikely to be available in more than a small handful of languages, and will remain outside the scope of a universal corpus, at least for the foreseeable future.


\section{Data Sources}

Although data size matters in general NLP \cite{banko2001scaling}, \emph{universality} is the utmost priority in NLP for the Universal Corpus initiative (aka Human Language Project; Abney and Bird, 2010; 2011). Hence, the uniWaC approach to data collection is based on focused/dedicated crawling unlike previous WaC word/URL list seeding. We manually selected sites that contains texts from a large variety of languages and wrote scripts to crawl and clean specific pages from the different data sources. The crawling and cleaning was done simultaneously. 

The resulting files were saved as plaintext file with \texttt{UTF-8} encoding. We used one directory for each data source and one file per language. The parallel data (from ODIN and Omniglot) are saved as tab-delimited files, one line per sentence, one column per language. The monolingual data (from UDHR and Wikipedia) are saved as newline delimited plaintext file, each line denoting a paragraph or sentence and each document separated by an empty line. The files are saved with the following naming convention to allow easy access, the filename contains the data source and its ISO 639-3 code, e.g. \texttt{odin-iii.txt} contains the \emph{ODIN} data for the \emph{Nuosu} language. [File structure is not relevant.]

API which allows access according to Abney and Bird.  However, maintaining a plain text file ensures that the data will be easily accessible, even if the surrounding infrastructure changes. [Seven dimensions]

Our resulting corpus runs the full gamut of texts outlined by Abney and Bird, from single-language text (Wikipedia) to parallel text (UDHR and Omniglot) to IGTs (ODIN). ...

In the follow subsections, we describe the preprocessing required to convert each data source into a standardised form.

\subsection{ODIN}

ODIN (The Online Database of Interlinear Text) is a repository of IGTs extracted from scholarly documents \cite{lewis2006odin,lewis2010odin}.  Compared to other resources, it is notable for the breadth of languages included, and the level of linguistic annotation; however, the data requires further processing to bring it in line with the proposed format for a universal corpus.

The ODIN data is easily accessible in XML format from the online database\footnote{http://odin.linguistlist.org/download}, where data for each language is saved in a separate XML file and the Interlinear Glossed Texts (IGTs) are encoded in the \texttt{<igt><example>...</example></igt>} tags. Each XML file is saved under a filename that matches the respective language code. While cleaning the data, filenames that does not adhere to ISO 639-3\footnote{http://www-01.sil.org/iso639-3} were excluded in the uniWaC compilation. [Huh? Should this really happen? - the codes are given on their website and look valid to me...]

\begin{minipage}{\columnwidth}

a. \quad o lesu mai \\
\indent \qquad 2sg return here \\

\indent \qquad `\emph{You return here.}' \\
\centerline{Figure 1: IGT from ODIN.}
\end{minipage}

\begin{minipage}{\columnwidth}
\begin{lstlisting}[language=XML]
<igt>
  <example>
    <line>21 a. o lesu mai</line>
    <line>2sg return here</line>
    <line>`You return here.'</line>
  </example>
</igit>
\end{lstlisting} 
\centerline{Figure 2: Fijian IGT from ODIN.}
\end{minipage}
\\ \\
\noindent The IGTs from a web document usually follow the Leipzig Glossing Rules, where the first line is the source language text, the second line contains the word/morphemic equivalence and the third line is an English gloss. From the ODIN XML format, an IGT as of Figure 1 will be represented as in an XML snippet as in Figure 2. Eventually, we need to clean and extract (i) the source text `\emph{o lesu mai}' without the preceding index `\emph{a.}' and (ii) the target language gloss without the quotation marks `\emph{You return here}'.
\begin{minipage}{\columnwidth}
\begin{lstlisting}[language=XML]
<igt>
  <example>
    <line>(69) na-Na-tmi-kwalca-t 
    Yimas (Foley 1991)</line>
    <line>3sgA-1sgO-say-rise-PERF
    </line>
    <line>`She woke me up' 
    (by verbal action)</line>
  </example>
</igit>
\end{lstlisting} 
\smallskip
\centerline{Figure 3: Yimas IGT from ODIN.}
\end{minipage}
\\ \\
\noindent The primary problem in extracting the source text is a lack of consistency in the IGTs. In the above examples, the sentence is introduced by a letter or number, which needs to be removed; however, the form of such indexing elements varies. In addition, the source line in Figure 3 includes the language name, and a citation for the example, which introduces noise into the corpus.

Lewis and Xia (2010) noted that it is not uncommon for IGTs found in documents to deviate from the standard convention; when compiling the ODIN data, they attempted a regex-based approached and reported 59\% F-score. Similar to their regex approaches for extracting IGT, we cleaned the source line with regexes and kept the ODIN data with two levels of `cleanliness': [Have we considered that the number of elements in the source line and gloss line should match...?]
\\ \\
\begin{minipage}{\columnwidth}
\begin{itemize}
\item \emph{Cleaner}: Removed (i) all heading and trailing text embedded in square or rounded brackets and (ii) heading double character token ending with bracket or fullstop.
\begin{itemize}
\item[(i)]
\begin{Verbatim}
^(?\s?\w{1,5}\s*[):.]\s*
\end{Verbatim}
\item[(ii)] 
\begin{Verbatim}
[\[\(].{1,}[\]\)]
\end{Verbatim}
\end{itemize}
\item \emph{Cleanest}: Only source lines without punctuation.
\end{itemize}
\end{minipage}
\\ \\

\noindent The original version of the ODIN data contains XML files for 1275 languages, while the cleaner version of ODIN contains IGTs for 1042 languages and the cleanest version contains IGTs for 402 languages. The drop from 1275 to 1042 languages was largely because {\color{red} XXXX} XML files from the original ODIN data had used language codes that were not in ISO 639-3 and for {\color{red} XXXX} other files, the \texttt{<igt>...</igt>} tags were missing. 

\subsection{Omniglot}

The Omniglot website\footnote{htp://www.omniglot.com/} is an online encyclopedia of writing systems and languages. We chose to extract information from pages on \emph{`Useful foreign phrases'} and the \emph{`Tower of Babel'} story, both of which give us a parallel data in a reasonably large number of languages.

[The following discussion is far too technical.]

\begin{itemize}[noitemsep]
\item \url{www.omniglot.com/language/phrases/*} 
\item \url{www.omniglot.com/babel/*}
\end{itemize}

\noindent The \emph{`Useful foreign phrases'} page contains parallel phrases in embedded within the \texttt{<th><tr>...</tr><th>} tags. The English phrase and foreign language phrase are encoded separately with \texttt{<td>...</td>} tags. Figure 4 shows an instance of an Omniglot \emph{Useful foreign phrase} page. Since the HTML page was designed as WYSIWYG, several pages have non-textual elements within the \texttt{<tr>...</tr>} tags for aesthetics. For example, the non-breaking space, \texttt{<td>\&nbsp;</td>} in Fig. 4; the extracted text was properly converted into plaintext format using the HTMLParser\footnote{http://docs.python.org/2/library/htmlparser.html} module, during which the non textual elements would have been cleaned. 

The same process of crawling URLs and cleaning was performed on the \emph{`Tower of Babel'} pages and the only difference was that the texts were embedded in a \texttt{<ol><li>...</li></ol>} tags instead.

\begin{minipage}{\columnwidth}
\begin{lstlisting}[language=XML]
<th>
  ...
  <tr>
    <td>I don't understand</td>
    <td>No appo cumpresu nutta</td>
    <td>&nbsp;</td>
  </tr>
  <tr>
    <td>I don't understand</td>
    <td>Non d'isco</td>
    <td>&nbsp;</td>
  </tr>
  ...
<th>
\end{lstlisting} 
\centerline{Figure 4: A \emph{Useful Foreign Phrase} page}
\end{minipage}
\smallskip

\noindent One problem with this data is that only the language name is given, not the ISO 639-3 code. To resolve this issue, we converted language names to codes using information from the SIL website.\footnote{http://www-01.sil.org/iso639-3/iso-639-3.tab}

\subsection{Universal Declaration of Human Rights}

The Universal Declaration of Human Rights (UDHR) is a document released by the United Nations, which has been translated into a wide variety of languages. Although there was a pre-compiled version of the UDHR data from the Natural Language ToolKit (NLTK) corpora distribution\footnote{http://nltk.googlecode.com/svn/trunk/nltk\_data/index.xml}, the distribution was laden with encoding problems during their conversion from pdf to plaintext format. Instead, we used the plaintext files available from the Unicode website\footnote{http://unicode.org/udhr/d/}, which are free of encoding issues. The first four lines of each file records metadata, and the rest is the translation of the UDHR.

[This paragraph seems completely incongruous] Although the redistribution of previous compiled data is not scientifically attractive but redistribution/recompilation of data to improve the usability and quality of the data is worth mentioning. For instance, the SETimes corpus first compiled by Tyers and Alperen (2010) was cleaned and redistributed by \newcite{ljubesic2013ner}. [This is not mentioned in the reference...]

\subsection{Wikipedia}

Tapping on the crowd-sourced Wikipedia articles for NLP forms a crucial part of developing data-driven NLP tools and application ({\color{red}{citation neeeded}}). [It might be popular, but it's not crucial...] To automatically download the Wikipedia dumps, we have scripted the \texttt{wget} with ISO 639-2 language code as a variable (\texttt{\$I}) in a shell script, as such: [This is too technical]

\smallskip
\noindent \texttt{\$ wget http://download.wikimedia.
org/\$Iwiki/latest/itwiki-latest-
pages-articles.xml.bz2}
\medskip

\noindent One major issue with using the Wikipedia dump is the sheer size and extracting text from a glut of Wikipedia markup. To convert compressed Wikipedia dumps to textfiles, we used the WikiExtractor \footnote{http://medialab.di.unipi.it/wiki/Wikipedia\_Extractor} tool.

After converting them into textfiles, we used the following regexes to delete residual Wikipedia markup and so-called ``magic words"\footnote{http://en.wikipedia.org/wiki/Help:Magic\_words}.

\subsection{Leipzig Corpora Collection}

Do we want to mention this?


\section{Copyright Issues}

Creative Commons Licence, etc.


\section{Representation and Universality}

Number of languages, size of corpus, etc.


\section{Language Clustering}

This would be an example application, if we have time...


\section{Future Work}

Other corpora we could work on...


\section{Conclusion}

Summarise what we've done!


\bibliography{references}

\end{document}
